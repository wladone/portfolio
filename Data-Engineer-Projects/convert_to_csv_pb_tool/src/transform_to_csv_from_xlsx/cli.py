from __future__ import annotations

from pathlib import Path
from typing import Dict, Optional

import typer
from rich.console import Console

from .transform import parse_and_backfill_times, transform_workbook

app = typer.Typer(add_completion=False, help="Convert regatta Excel workbooks into Power BI friendly tables.")
console = Console()

SCHEMA_EXPECTATIONS: Dict[str, list[str]] = {
    "fact_results": [
        "FactKey",
        "RaceKey",
        "Regatta Number",
        "Race Number",
        "Division",
        "Sail Number",
        "Boat",
        "Class (boat type)",
        "Yacht Club",
        "Handicap/Rating",
        "Rank",
        "Points",
        "Elapsed_HHMMSS",
        "Elapsed_sec",
        "Corrected_HHMMSS",
        "Corrected_sec",
        "IsCorrectedMissing",
        "IsTieWithinDivision",
    ],
    "dim_boat": [
        "Sail Number",
        "Boat",
        "Class (boat type)",
        "Yacht Club",
        "Handicap/Rating",
    ],
    "dim_race": ["RaceKey", "Regatta Number", "Race Number"],
    "dim_division": ["Division"],
}

UNIQUE_EXPECTATIONS: Dict[str, str] = {
    "fact_results": "FactKey",
    "dim_boat": "Sail Number",
    "dim_race": "RaceKey",
    "dim_division": "Division",
}

FK_EXPECTATIONS = [
    ("Sail Number", "dim_boat"),
    ("RaceKey", "dim_race"),
    ("Division", "dim_division"),
]


def _info(message: str) -> None:
    console.print(f"[bold cyan]INFO[/] {message}")


def _warn(message: str) -> None:
    console.print(f"[bold yellow]WARN[/] {message}")


def _error(message: str) -> None:
    console.print(f"[bold red]ERROR[/] {message}")



def _build_dataset_readme(summary: Dict[str, int], workbook: Path, sheet: str) -> str:
    return f"""# Power BI dataset

Generated by `regatta-to-powerbi` from `{workbook.name}` (sheet: `{sheet}`). Load the CSV files in this folder into Power BI Desktop and relate them as shown below.

## Files and rows
- fact_results.csv - {summary.get('fact_results', 0)} rows
- dim_boat.csv - {summary.get('dim_boat', 0)} rows
- dim_race.csv - {summary.get('dim_race', 0)} rows
- dim_division.csv - {summary.get('dim_division', 0)} rows

## Import steps
1. Open Power BI Desktop, choose **Get Data -> Text/CSV**, and load each file in this folder.
2. Ensure all CSVs use UTF-8 and detect data types automatically.
3. Create model relationships exactly as listed below.

## Relationships
- fact_results[Sail Number] -> dim_boat[Sail Number] (many-to-one)
- fact_results[RaceKey] -> dim_race[RaceKey] (many-to-one)
- fact_results[Division] -> dim_division[Division] (many-to-one)

## Suggested DAX measures
Boats = DISTINCTCOUNT(fact_results[Sail Number])
Races = DISTINCTCOUNT(fact_results[Race Number])
Regattas = DISTINCTCOUNT(fact_results[Regatta Number])
Winners = CALCULATE(COUNTROWS(fact_results), fact_results[Rank] = 1)
Avg Points = AVERAGE(fact_results[Points])
Median Points = MEDIAN(fact_results[Points])
Avg Corrected (sec) = AVERAGE(fact_results[Corrected_sec])
% Missing Corrected = DIVIDE(CALCULATE(COUNTROWS(fact_results), fact_results[IsCorrectedMissing] = TRUE()), COUNTROWS(fact_results))
"""


def _select_workbook(
    xlsx_path: Optional[Path], in_dir: Path, workbook_name: Optional[str]
) -> Path:
    if xlsx_path is not None:
        candidate = xlsx_path.expanduser()
        if not candidate.exists():
            raise FileNotFoundError(f"Workbook not found: {candidate}")
        if candidate.is_dir():
            raise IsADirectoryError(f"Expected a file but got directory: {candidate}")
        return candidate

    search_dir = in_dir.expanduser()
    if not search_dir.exists() or not search_dir.is_dir():
        raise FileNotFoundError(f"Input directory does not exist: {search_dir}")

    if workbook_name:
        candidate = (search_dir / workbook_name).resolve()
        if not candidate.exists():
            raise FileNotFoundError(f"Workbook not found in input directory: {candidate}")
        if candidate.is_dir():
            raise IsADirectoryError(f"Expected a file but got directory: {candidate}")
        return candidate

    matches = sorted(search_dir.glob("*.xlsx"))
    if not matches:
        raise FileNotFoundError(f"No .xlsx files found in {search_dir}")
    if len(matches) > 1:
        joined = ", ".join(match.name for match in matches)
        raise RuntimeError(
            "Multiple .xlsx files found; pass --workbook-name to choose one: " + joined
        )
    return matches[0]


@app.command()
def main(
    xlsx: Optional[Path] = typer.Option(
        None,
        "--xlsx",
        "--xlsx-path",
        help="Explicit path to an Excel workbook; overrides --in-dir/--workbook-name and enables time backfill.",
    ),
    in_dir: Path = typer.Option(
        Path("input"),
        "--in-dir",
        "-i",
        help="Directory that contains the Excel workbook when --xlsx is omitted.",
    ),
    workbook_name: Optional[str] = typer.Option(
        None,
        "--workbook-name",
        "-w",
        help="Excel file name inside --in-dir when multiple workbooks exist.",
    ),
    sheet: str = typer.Option("Regatta Results", "--sheet", "-s", help="Worksheet name."),
    out_dir: Path = typer.Option(
        Path("powerbi_dataset"),
        "--out-dir",
        "-o",
        help="Output directory for CSVs.",
    ),
    report: Path = typer.Option(
        Path("REPORT.txt"),
        "--report",
        "-r",
        help="Validation report filename (relative to --out-dir unless absolute).",
    ),
) -> None:
    """Convert regatta Excel workbooks into Power BI friendly tables."""
    try:
        workbook = _select_workbook(xlsx, in_dir, workbook_name)
    except Exception as exc:  # pragma: no cover
        _error(str(exc))
        raise typer.Exit(code=1) from exc

    _info(f"Workbook: {workbook}")
    _info(f"Sheet: {sheet}")
    _info(f"Output directory: {out_dir}")

    try:
        frames, _ = transform_workbook(workbook, sheet)
    except Exception as exc:  # pragma: no cover
        _error(str(exc))
        raise typer.Exit(code=1) from exc

    fact_before = frames["fact_results"]
    backfill_path = str(xlsx) if xlsx else None
    fact_after = parse_and_backfill_times(fact_before, backfill_path)
    metrics = fact_after.attrs.get("time_completeness", {})
    before_elapsed = metrics.get("before_elapsed")
    before_corrected = metrics.get("before_corrected")
    after_elapsed = metrics.get("after_elapsed")
    after_corrected = metrics.get("after_corrected")
    if before_elapsed is None:
        before_elapsed = float(fact_before["Elapsed_sec"].notna().mean() * 100) if len(fact_before) else 0.0
    if before_corrected is None:
        before_corrected = float(fact_before["Corrected_sec"].notna().mean() * 100) if len(fact_before) else 0.0
    if after_elapsed is None:
        after_elapsed = float(fact_after["Elapsed_sec"].notna().mean() * 100) if len(fact_after) else 0.0
    if after_corrected is None:
        after_corrected = float(fact_after["Corrected_sec"].notna().mean() * 100) if len(fact_after) else 0.0
    frames["fact_results"] = fact_after

    report_lines: list[str] = []

    report_lines.append("Schema Checks")
    for dataset, expected_columns in SCHEMA_EXPECTATIONS.items():
        frame = frames[dataset]
        missing_cols = [col for col in expected_columns if col not in frame.columns]
        if missing_cols:
            _warn(f"{dataset} missing columns: {', '.join(missing_cols)}")
            report_lines.append(f"- {dataset}: FAIL missing {', '.join(missing_cols)}")
        else:
            _info(f"{dataset} schema OK")
            report_lines.append(f"- {dataset}: OK")

    report_lines.append("")
    report_lines.append("Uniqueness")
    for dataset, column in UNIQUE_EXPECTATIONS.items():
        frame = frames[dataset]
        series = frame[column]
        if series.is_unique:
            _info(f"{dataset}.{column} is unique")
            report_lines.append(f"- {dataset}.{column}: OK")
        else:
            duplicates = series[series.duplicated()].dropna().unique().tolist()
            sample = ", ".join(map(str, duplicates[:10])) if duplicates else "N/A"
            _warn(f"{dataset}.{column} duplicates: {sample}")
            report_lines.append(f"- {dataset}.{column}: FAIL duplicates {sample}")

    report_lines.append("")
    report_lines.append("Foreign Key Coverage")
    for column, parent_name in FK_EXPECTATIONS:
        child_values = frames["fact_results"][column].dropna().tolist()
        parent_values = frames[parent_name][column].dropna().tolist()
        missing_values = sorted({str(value) for value in child_values} - {str(value) for value in parent_values})
        if not missing_values:
            _info(f"{column} values fully covered by {parent_name}")
            report_lines.append(f"- {column} -> {parent_name}: OK")
        else:
            sample = ", ".join(missing_values[:10])
            _warn(f"{column} values missing in {parent_name}: {sample}")
            report_lines.append(f"- {column} -> {parent_name}: FAIL missing {sample}")

    report_lines.append("")
    report_lines.append("Time Completeness")
    elapsed_line = (
        f"- Elapsed_sec completeness: before {before_elapsed:.1f}% / after {after_elapsed:.1f}%"
    )
    corrected_line = (
        f"- Corrected_sec completeness: before {before_corrected:.1f}% / after {after_corrected:.1f}%"
    )
    report_lines.extend([elapsed_line, corrected_line])
    _info(elapsed_line[2:])
    _info(corrected_line[2:])

    needs_more = after_elapsed < 95.0 or after_corrected < 95.0
    if needs_more:
        if not xlsx:
            message = "Time fields still incomplete. Provide --xlsx to backfill from the source workbook."
            _warn(message)
            report_lines.append("- Remaining missing FactKeys (<95% coverage): advise --xlsx")
        else:
            remaining_mask = fact_after["Elapsed_sec"].isna() & fact_after["Corrected_sec"].isna()
            missing_keys = fact_after.loc[remaining_mask, "FactKey"].dropna().astype(str).head(10).tolist()
            sample = ", ".join(missing_keys) if missing_keys else "None"
            _warn(f"Remaining rows with missing times (first 10 FactKeys): {sample}")
            report_lines.append(f"- Remaining missing FactKeys (<95% coverage): {sample}")
    else:
        _info("Time columns >=95% complete after backfill")
        report_lines.append("- Remaining missing FactKeys: None")

    out_dir.mkdir(parents=True, exist_ok=True)

    summary: Dict[str, int] = {}
    for name, frame in frames.items():
        csv_path = out_dir / f"{name}.csv"
        frame.to_csv(csv_path, index=False, encoding="utf-8")
        summary[name] = len(frame)
        _info(f"Wrote {len(frame)} rows -> {csv_path.name}")

    missing_pct_after = float(fact_after["IsCorrectedMissing"].mean() * 100) if not fact_after.empty else 0.0
    if missing_pct_after > 0:
        remaining = fact_after.loc[fact_after["IsCorrectedMissing"], "FactKey"].astype(str).head(10).tolist()
        sample = ", ".join(remaining) if remaining else "None"
        _warn(
            f"Corrected times still missing for {missing_pct_after:.1f}% of rows (sample FactKeys: {sample})"
        )

    dataset_readme = _build_dataset_readme(summary, workbook, sheet)
    (out_dir / "README.md").write_text(dataset_readme, encoding="utf-8")

    report_path = report if report.is_absolute() else out_dir / report
    report_path.parent.mkdir(parents=True, exist_ok=True)
    report_content = "\n".join(report_lines).rstrip() + "\n"
    report_path.write_text(report_content, encoding="utf-8")
    _info(f"Validation report -> {report_path}")

    console.print("[bold green]Done.[/]")


if __name__ == "__main__":
    app()
