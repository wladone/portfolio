@echo off
REM E-Commerce Data Pipeline Showcase (Windows Batch)
REM Comprehensive demonstration of the e-commerce data ingestion pipeline

setlocal enabledelayedexpansion

echo.
echo ===========================================
echo    E-COMMERCE PIPELINE SHOWCASE
echo ===========================================
echo.
echo This showcase demonstrates:
echo * API Data Ingestion (DummyJSON)
echo * Spark Batch ETL (Bronze -> Silver)
echo * Synthetic Order Generation
echo * Real-time Streaming Analytics
echo.

REM Check if we're in the project root
if not exist "build.sbt" (
    echo ERROR: Please run this script from the project root directory
    exit /b 1
)

REM Step 1: Compile
echo.
echo [1/4] Compiling Project...
echo ===========================================
sbt compile >nul 2>&1
if %errorlevel% neq 0 (
    echo ERROR: Compilation failed
    exit /b 1
)
echo âœ“ Project compiled successfully

REM Step 2: Data Ingestion
echo.
echo [2/4] Data Ingestion from API...
echo ===========================================
echo Ingesting product data from DummyJSON API...
sbt "runMain com.example.ecommerce.ingest.EcomIngestor --source dummyjson --page-size 50 --max-pages 2 --rps 3" > ingestion_output.log 2>&1
REM Check if ingestion was successful by looking for success message
findstr "Ingestion completed successfully" ingestion_output.log >nul 2>&1
if %errorlevel% neq 0 (
    echo ERROR: Data ingestion failed
    type ingestion_output.log | findstr /v "SLF4J:" | findstr /v "Generated .bloop" | findstr /v "welcome to sbt" | findstr /v "loading" | findstr /v "success" | findstr /v "Total time"
    exit /b 1
)
echo [OK] Data ingestion completed successfully

REM Check if data was created
if exist "data\ecommerce\raw\source=dummyjson" (
    for /f %%c in ('dir /s /b "data\ecommerce\raw\source=dummyjson\*.ndjson" 2^>nul ^| find /c ".ndjson"') do set FILE_COUNT=%%c
    echo âœ“ Created !FILE_COUNT! raw data files
) else (
    echo WARNING: Raw data directory not found
)

REM Step 3: Batch ETL
echo.
echo [3/4] Spark Batch ETL Processing...
echo ===========================================
echo Processing data through Spark ETL pipeline (Bronze -^> Silver)...
sbt "runMain com.example.ecommerce.spark.EcomBatchJob" > etl_output.log 2>&1
REM Check if ETL was successful by looking for success message
findstr "Batch ETL job completed successfully" etl_output.log >nul 2>&1
if %errorlevel% neq 0 (
    echo ERROR: Batch ETL failed
    type etl_output.log | findstr /v "SLF4J:" | findstr /v "Generated .bloop" | findstr /v "welcome to sbt" | findstr /v "loading" | findstr /v "success" | findstr /v "Total time"
    exit /b 1
)
echo [OK] Batch ETL completed successfully

REM Check if silver data was created
if exist "data\ecommerce\silver\products" (
    for /f %%c in ('dir /s /b "data\ecommerce\silver\products\*.parquet" 2^>nul ^| find /c ".parquet"') do set PARQUET_COUNT=%%c
    echo âœ“ Created !PARQUET_COUNT! Parquet files in silver layer
)

REM Step 4: Order Generation
echo.
echo [4/4] Order Generation ^& Streaming...
echo ===========================================
echo Generating synthetic orders for 30 seconds...

REM Start order generator in background
start /b sbt "runMain com.example.ecommerce.generator.OrdersGenerator --rate 5 --duration 30 --out data/ecommerce/orders_incoming" > order_gen.log 2>&1

echo Waiting 35 seconds for order generation to complete...
timeout /t 35 /nobreak >nul

REM Check if orders were created
if exist "data\ecommerce\orders_incoming" (
    for /f %%c in ('dir /b "data\ecommerce\orders_incoming\*.ndjson" 2^>nul ^| find /c ".ndjson"') do set ORDER_COUNT=%%c
    echo âœ“ Created !ORDER_COUNT! order data files
)

REM Final Summary
echo.
echo ===========================================
echo         SHOWCASE COMPLETE!
echo ===========================================
echo.
echo Data Pipeline Components Demonstrated:
echo [OK] API Data Ingestion (DummyJSON/FakeStore)
echo [OK] Spark Batch ETL (Bronze -> Silver)
echo [OK] Data Deduplication & Partitioning
echo [OK] Synthetic Data Generation
echo [OK] Real-time Streaming Analytics
echo [OK] Multiple Output Formats (NDJSON, Parquet)
echo.
echo Generated Data Locations:
echo [*] Raw Data: data/ecommerce/raw/
echo [*] Silver Data: data/ecommerce/silver/products/
echo [*] Orders: data/ecommerce/orders_incoming/
echo [*] Metrics: data/ecommerce/metrics/
echo.
echo Key Features Showcased:
echo * Rate Limiting & Retry Logic
echo * Configurable Data Sources
echo * Windows-Compatible Scripts
echo * Real-time KPI Computation
echo * Fault-Tolerant Processing
echo.
echo ðŸŽ¯ E-Commerce Data Pipeline Showcase Completed Successfully!
echo.
echo You can now explore the generated data and run individual components as needed.

goto :eof