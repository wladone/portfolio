# âš™ï¸ Data Pipeline with Apache Airflow & PostgreSQL
**Author:** Vlad Gabriel Popescu  
**Role:** Junior Data Engineer / Data Analyst  

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.7.3-blue)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-ready-blue)

## ğŸ“Œ Description
Daily ETL pipeline that extracts data from a public API, transforms it with Pandas, and loads it into PostgreSQL via Airflow.

---

## ğŸ“‚ Project Structure
```
Data-Pipeline-Airflow-Postgres/
â”‚â”€â”€ README.md
â”‚â”€â”€ dags/
â”‚   â”œâ”€â”€ api_to_postgres_dag.py
â”‚â”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚â”€â”€ docker-compose.yml
```

---

## ğŸ“œ DAG Explanation
- `extract_data()` downloads JSON from API and writes CSV to `/tmp/etl/posts.csv`
- `load_data()` inserts records into `posts` table using psycopg2
- DAG runs `@daily`, `catchup=False`

---

## ğŸš€ Run in Docker
```bash
docker compose up -d
# Airflow UI: http://localhost:8080 (user/pass: admin/admin)
# Enable DAG `api_to_postgres` and trigger a manual run.
docker compose down
```

---

## ğŸ“¸ Screenshots
_Add Airflow DAG images to `screenshots/`._

---

## ğŸ“œ License
MIT License (see `LICENSE.md`)
