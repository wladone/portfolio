# âš¡ Real-Time Streaming with Kafka & Spark Structured Streaming
**Author:** Vlad Gabriel Popescu  
**Role:** Junior Data Engineer / Big Data Developer  

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Kafka](https://img.shields.io/badge/Kafka-ready-black)
![Spark](https://img.shields.io/badge/Spark-ready-orange)

## ðŸ“Œ Description
Real-time ingestion with Apache Kafka and processing with Spark Structured Streaming. This setup prints messages to console (easy to demo).

---

## ðŸ“‚ Project Structure
```
Real-Time-Kafka-Spark/
â”‚â”€â”€ README.md
â”‚â”€â”€ producer/
â”‚   â”œâ”€â”€ kafka_producer.py
â”‚â”€â”€ consumer/
â”‚   â”œâ”€â”€ spark_streaming_job.py
â”‚â”€â”€ configs/
â”‚   â”œâ”€â”€ kafka_config.json
â”‚â”€â”€ docker-compose.yml
```

---

## ðŸ“œ Code Explanation
- **Producer** sends 10 JSON messages into `sensor_data` topic
- **Spark consumer** reads from Kafka and prints parsed messages to console

---

## ðŸš€ Run in Docker
```bash
docker compose up -d
# Create topic is handled dynamically by producer send; if needed you can also create via kafka-topics.sh
docker exec -it spark spark-submit /opt/spark-apps/consumer/spark_streaming_job.py
# In another terminal you can see producer logs in `docker compose logs -f producer`
docker compose down
```

---

## ðŸ“¸ Screenshots
_Add images to `screenshots/`._

---

## ðŸ“œ License
MIT License (see `LICENSE.md`)
