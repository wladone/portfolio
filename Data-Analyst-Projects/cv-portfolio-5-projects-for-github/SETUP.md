# ğŸ›  Portofoliu Proiecte Data & BI â€“ Vlad Gabriel Popescu

Acest document conÈ›ine instrucÈ›iuni pentru rularea separatÄƒ a fiecÄƒruia dintre cele 5 proiecte incluse Ã®n portofoliu.
Fiecare proiect are propriul `README.md` cu explicaÈ›ii linie-cu-linie È™i paÈ™i Docker.

## ğŸ“‚ Structura Proiectelor
1. `Sales-Analytics-Dashboard` â€“ Dashboard Power BI cu sursÄƒ SQL.
2. `Data-Pipeline-Airflow-Postgres` â€“ ETL zilnic API â†’ PostgreSQL cu Apache Airflow.
3. `PySpark-Batch-Processing` â€“ Procesare batch cu PySpark (local/Spark container).
4. `FastAPI-Postgres-DataAPI` â€“ API REST (FastAPI) pentru date financiare.
5. `Real-Time-Kafka-Spark` â€“ Streaming Ã®n timp real cu Kafka & Spark.

---

## 1ï¸âƒ£ Sales Analytics Dashboard
```bash
cd Sales-Analytics-Dashboard
docker compose up -d
# ConecteazÄƒ Power BI Desktop la PostgreSQL: host localhost, db salesdb, user postgres, pass postgres
# La final:
docker compose down
```

## 2ï¸âƒ£ Data Pipeline Airflow + Postgres
```bash
cd Data-Pipeline-Airflow-Postgres
docker compose up -d
# UI Airflow: http://localhost:8080  (creeazÄƒ user dacÄƒ e necesar)
# ActiveazÄƒ DAG-ul `api_to_postgres` È™i ruleazÄƒ manual prima execuÈ›ie.
docker compose down
```

## 3ï¸âƒ£ PySpark Batch Processing
```bash
cd PySpark-Batch-Processing
# Variante:
# A) Spark container:
docker compose up -d
docker exec -it pyspark-batch-processing-spark-1 spark-submit /opt/spark-apps/notebooks/batch_job.py
# B) Local (dacÄƒ ai Spark instalat):
# spark-submit notebooks/batch_job_local.py
docker compose down
```

## 4ï¸âƒ£ FastAPI + Postgres API
```bash
cd FastAPI-Postgres-DataAPI
docker compose up --build -d
# API Docs: http://localhost:8000/docs
docker compose down
```

## 5ï¸âƒ£ Real-Time Kafka + Spark
```bash
cd Real-Time-Kafka-Spark
docker compose up -d
# CreeazÄƒ topicul:
docker exec -it kafka /opt/kafka/bin/kafka-topics.sh --create --topic sensor_data --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
# RuleazÄƒ producerul (Ã®n containerul producer) È™i consumatorul Spark:
docker exec -it producer python /app/producer/kafka_producer.py
docker exec -it spark spark-submit /opt/spark-apps/consumer/spark_streaming_job.py
docker compose down
```

---

## ğŸ›‘ Note
- RuleazÄƒ proiectele **independent**. OpreÈ™te containerele unui proiect Ã®nainte de a porni urmÄƒtorul.
- DacÄƒ Ã®ntÃ¢mpini probleme de permisiuni pe volume, pe Linux/Mac poÈ›i folosi: `chmod -R 777 .` Ã®n folderul proiectului.
