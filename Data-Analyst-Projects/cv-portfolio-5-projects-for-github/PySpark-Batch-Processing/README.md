# ðŸš€ PySpark Batch Processing â€“ Databricks/Local
**Author:** Vlad Gabriel Popescu  
**Role:** Junior Data Engineer  

![Status](https://img.shields.io/badge/status-active-brightgreen)
![PySpark](https://img.shields.io/badge/PySpark-ready-orange)

## ðŸ“Œ Description
Process CSV datasets with PySpark and output optimized Parquet files.

---

## ðŸ“‚ Project Structure
```
PySpark-Batch-Processing/
â”‚â”€â”€ README.md
â”‚â”€â”€ notebooks/
â”‚   â”œâ”€â”€ batch_job.py
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ large_dataset.csv
â”‚â”€â”€ output/               # Parquet output
â”‚â”€â”€ docker-compose.yml
```

---

## ðŸ“œ Code Explanation
- Reads `/data/large_dataset.csv` with header and inferred schema
- Renames `old_column` â†’ `new_column`, drops NA rows
- Writes results to `/output/parquet_data`

---

## ðŸš€ Run in Docker
```bash
docker compose up -d
docker exec -it pyspark-batch spark-submit /opt/spark-apps/batch_job.py
# Parquet files appear under ./output/parquet_data
docker compose down
```

---

## ðŸ“¸ Screenshots
_Add images to `screenshots/`._

---

## ðŸ“œ License
MIT License (see `LICENSE.md`)
